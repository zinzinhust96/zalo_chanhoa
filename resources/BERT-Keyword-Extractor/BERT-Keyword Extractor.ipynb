{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword-Extraction using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use BERT Token Classification Model to extract keyword tokens from a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT.\n",
    "\n",
    "Convert Sem-Eval 2010 keyword recognition dataset to BIO format dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"maui-semeval2010-train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = sorted([f for f in os.listdir(train_path) if not f.endswith(\"-justTitle.txt\") and not f.endswith(\".key\") and not f.endswith(\"-CrowdCountskey\")])\n",
    "key = sorted([f for f in os.listdir(train_path) if f.endswith(\".key\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filekey = dict()\n",
    "for i, k in enumerate(txt):\n",
    "    filekey[key[i]] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(key):\n",
    "    # TODO: understand this\n",
    "    sentences = \"\"\n",
    "    for line in open(train_path + \"/\" + filekey[key], 'r'):\n",
    "        sentences += (\" \" + line.rstrip())\n",
    "    tokens = sent_tokenize(sentences)\n",
    "    key_file = open(train_path + \"/\" + str(key),'r')\n",
    "    keys = [line.strip() for line in key_file]\n",
    "    key_sent = []\n",
    "    labels = []\n",
    "    for token in tokens:\n",
    "        z = ['O'] * len(token.split())\n",
    "        for k in keys:\n",
    "            if k in token:\n",
    "                \n",
    "                if len(k.split())==1:\n",
    "                    try:\n",
    "                        z[token.lower().split().index(k.lower().split()[0])] = 'B'\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                elif len(k.split())>1:\n",
    "                    try:\n",
    "                        if token.lower().split().index(k.lower().split()[0]) and token.lower().split().index(k.lower().split()[-1]):\n",
    "                            z[token.lower().split().index(k.lower().split()[0])] = 'B'\n",
    "                            for j in range(1, len(k.split())):\n",
    "                                z[token.lower().split().index(k.lower().split()[j])] = 'I'\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        for m, n in enumerate(z):\n",
    "            if z[m] == 'I' and z[m-1] == 'O':\n",
    "                z[m] = 'O'\n",
    "\n",
    "        if set(z) != {'O'}:\n",
    "            labels.append(z) \n",
    "            key_sent.append(token)\n",
    "    return key_sent, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8584 8584\n"
     ]
    }
   ],
   "source": [
    "sentences_ = []\n",
    "labels_ = []\n",
    "for key, value in filekey.items():\n",
    "    s, l = convert(key)\n",
    "    sentences_.append(s)\n",
    "    labels_.append(l)\n",
    "sentences = [item for sublist in sentences_ for item in sublist]\n",
    "labels = [item for sublist in labels_ for item in sublist]\n",
    "print(len(sentences), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {'B': 0, 'I': 1, 'O': 2}\n",
    "tags_vals = ['B', 'I', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 231508/231508 [00:01<00:00, 230339.91B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "23B/s]\u001b[A\n",
      " 81%|████████▏ | 331616256/407873900 [02:56<00:32, 2365234.79B/s]\u001b[A\n",
      " 81%|████████▏ | 331876352/407873900 [02:57<00:31, 2375003.79B/s]\u001b[A\n",
      " 81%|████████▏ | 332187648/407873900 [02:57<00:30, 2466259.19B/s]\u001b[A\n",
      " 82%|████████▏ | 332436480/407873900 [02:57<00:30, 2471698.48B/s]\u001b[A\n",
      " 82%|████████▏ | 332711936/407873900 [02:57<00:30, 2473495.66B/s]\u001b[A\n",
      " 82%|████████▏ | 332960768/407873900 [02:57<00:30, 2451583.90B/s]\u001b[A\n",
      " 82%|████████▏ | 333219840/407873900 [02:57<00:30, 2443683.73B/s]\u001b[A\n",
      " 82%|████████▏ | 333531136/407873900 [02:57<00:29, 2552732.35B/s]\u001b[A\n",
      " 82%|████████▏ | 333793280/407873900 [02:57<00:29, 2533586.62B/s]\u001b[A\n",
      " 82%|████████▏ | 334071808/407873900 [02:57<00:28, 2563961.89B/s]\u001b[A\n",
      " 82%|████████▏ | 334333952/407873900 [02:58<00:28, 2543847.11B/s]\u001b[A\n",
      " 82%|████████▏ | 334612480/407873900 [02:58<00:29, 2519414.76B/s]\u001b[A\n",
      " 82%|████████▏ | 334923776/407873900 [02:58<00:27, 2623572.91B/s]\u001b[A\n",
      " 82%|████████▏ | 335202304/407873900 [02:58<00:27, 2616169.15B/s]\u001b[A\n",
      " 82%|████████▏ | 335497216/407873900 [02:58<00:27, 2666208.34B/s]\u001b[A\n",
      " 82%|████████▏ | 335765504/407873900 [02:58<00:27, 2606770.47B/s]\u001b[A\n",
      " 82%|████████▏ | 336037888/407873900 [02:58<00:27, 2625267.21B/s]\u001b[A\n",
      " 82%|████████▏ | 336381952/407873900 [02:58<00:26, 2727527.73B/s]\u001b[A\n",
      " 83%|████████▎ | 336660480/407873900 [02:58<00:26, 2702166.70B/s]\u001b[A\n",
      " 83%|████████▎ | 336971776/407873900 [02:59<00:25, 2768450.07B/s]\u001b[A\n",
      " 83%|████████▎ | 337250304/407873900 [02:59<00:26, 2667906.13B/s]\u001b[A\n",
      " 83%|████████▎ | 337561600/407873900 [02:59<00:25, 2717468.34B/s]\u001b[A\n",
      " 83%|████████▎ | 337905664/407873900 [02:59<00:25, 2795943.48B/s]\u001b[A\n",
      " 83%|████████▎ | 338200576/407873900 [02:59<00:24, 2808794.23B/s]\u001b[A\n",
      " 83%|████████▎ | 338528256/407873900 [02:59<00:24, 2888228.56B/s]\u001b[A\n",
      " 83%|████████▎ | 338819072/407873900 [02:59<00:24, 2859956.79B/s]\u001b[A\n",
      " 83%|████████▎ | 339118080/407873900 [02:59<00:23, 2880968.69B/s]\u001b[A\n",
      " 83%|████████▎ | 339478528/407873900 [02:59<00:22, 3060217.02B/s]\u001b[A\n",
      " 83%|████████▎ | 339789824/407873900 [02:59<00:22, 2998534.86B/s]\u001b[A\n",
      " 83%|████████▎ | 340133888/407873900 [03:00<00:22, 3037251.00B/s]\u001b[A\n",
      " 83%|████████▎ | 340445184/407873900 [03:00<00:22, 2966939.54B/s]\u001b[A\n",
      " 84%|████████▎ | 340772864/407873900 [03:00<00:22, 3038507.88B/s]\u001b[A\n",
      " 84%|████████▎ | 341149696/407873900 [03:00<00:20, 3209420.26B/s]\u001b[A\n",
      " 84%|████████▎ | 341475328/407873900 [03:00<00:36, 1824858.63B/s]\u001b[A\n",
      " 84%|████████▍ | 341772288/407873900 [03:00<00:32, 2034268.28B/s]\u001b[A\n",
      " 84%|████████▍ | 342689792/407873900 [03:00<00:24, 2626518.74B/s]\u001b[A\n",
      " 84%|████████▍ | 343118848/407873900 [03:01<00:25, 2531588.01B/s]\u001b[A\n",
      " 84%|████████▍ | 343489536/407873900 [03:01<00:26, 2456266.41B/s]\u001b[A\n",
      " 84%|████████▍ | 343818240/407873900 [03:01<00:25, 2533906.72B/s]\u001b[A\n",
      " 84%|████████▍ | 344130560/407873900 [03:01<00:25, 2506351.65B/s]\u001b[A\n",
      " 84%|████████▍ | 344422400/407873900 [03:01<00:25, 2469256.27B/s]\u001b[A\n",
      " 85%|████████▍ | 344698880/407873900 [03:01<00:25, 2472778.41B/s]\u001b[A\n",
      " 85%|████████▍ | 344967168/407873900 [03:01<00:26, 2386555.24B/s]\u001b[A\n",
      " 85%|████████▍ | 345278464/407873900 [03:02<00:24, 2556860.57B/s]\u001b[A\n",
      " 85%|████████▍ | 345548800/407873900 [03:02<00:24, 2531244.04B/s]\u001b[A\n",
      " 85%|████████▍ | 345819136/407873900 [03:02<00:24, 2554279.99B/s]\u001b[A\n",
      " 85%|████████▍ | 346114048/407873900 [03:02<00:23, 2581488.85B/s]\u001b[A\n",
      " 85%|████████▍ | 346377216/407873900 [03:02<00:23, 2580819.36B/s]\u001b[A\n",
      " 85%|████████▍ | 346687488/407873900 [03:02<00:22, 2693437.32B/s]\u001b[A\n",
      " 85%|████████▌ | 346966016/407873900 [03:02<00:22, 2703021.69B/s]\u001b[A\n",
      " 85%|████████▌ | 347277312/407873900 [03:02<00:21, 2757069.21B/s]\u001b[A\n",
      " 85%|████████▌ | 347588608/407873900 [03:02<00:21, 2741801.52B/s]\u001b[A\n",
      " 85%|████████▌ | 347867136/407873900 [03:02<00:21, 2734369.26B/s]\u001b[A\n",
      " 85%|████████▌ | 348208128/407873900 [03:03<00:20, 2907098.78B/s]\u001b[A\n",
      " 85%|████████▌ | 348503040/407873900 [03:03<00:20, 2858081.00B/s]\u001b[A\n",
      " 86%|████████▌ | 348816384/407873900 [03:03<00:20, 2935318.89B/s]\u001b[A\n",
      " 86%|████████▌ | 349113344/407873900 [03:03<00:20, 2889629.74B/s]\u001b[A\n",
      " 86%|████████▌ | 349407232/407873900 [03:03<00:20, 2864486.87B/s]\u001b[A\n",
      " 86%|████████▌ | 349734912/407873900 [03:03<00:19, 2956986.17B/s]\u001b[A\n",
      " 86%|████████▌ | 350050304/407873900 [03:03<00:19, 3013274.91B/s]\u001b[A\n",
      " 86%|████████▌ | 350353408/407873900 [03:03<00:20, 2854861.15B/s]\u001b[A\n",
      " 86%|████████▌ | 350685184/407873900 [03:03<00:19, 2970471.02B/s]\u001b[A\n",
      " 86%|████████▌ | 351012864/407873900 [03:04<00:19, 2970228.44B/s]\u001b[A\n",
      " 86%|████████▌ | 351356928/407873900 [03:04<00:18, 3076453.98B/s]\u001b[A\n",
      " 86%|████████▌ | 351676416/407873900 [03:04<00:18, 3110136.31B/s]\u001b[A\n",
      " 86%|████████▋ | 351989760/407873900 [03:04<00:19, 2928009.17B/s]\u001b[A\n",
      " 86%|████████▋ | 352339968/407873900 [03:04<00:18, 3040050.57B/s]\u001b[A\n",
      " 86%|████████▋ | 352667648/407873900 [03:04<00:17, 3082449.20B/s]\u001b[A\n",
      " 87%|████████▋ | 353022976/407873900 [03:04<00:17, 3209899.89B/s]\u001b[A\n",
      " 87%|████████▋ | 353347584/407873900 [03:04<00:17, 3149938.14B/s]\u001b[A\n",
      " 87%|████████▋ | 353666048/407873900 [03:04<00:17, 3011998.42B/s]\u001b[A\n",
      " 87%|████████▋ | 354018304/407873900 [03:04<00:17, 3148627.11B/s]\u001b[A\n",
      " 87%|████████▋ | 354355200/407873900 [03:05<00:17, 3140586.51B/s]\u001b[A\n",
      " 87%|████████▋ | 354682880/407873900 [03:05<00:16, 3174777.35B/s]\u001b[A\n",
      " 87%|████████▋ | 355002368/407873900 [03:05<00:16, 3160160.24B/s]\u001b[A\n",
      " 87%|████████▋ | 355319808/407873900 [03:05<00:17, 3043523.78B/s]\u001b[A\n",
      " 87%|████████▋ | 355665920/407873900 [03:05<00:16, 3131427.41B/s]\u001b[A\n",
      " 87%|████████▋ | 356026368/407873900 [03:05<00:16, 3240039.89B/s]\u001b[A\n",
      " 87%|████████▋ | 356353024/407873900 [03:05<00:15, 3236392.58B/s]\u001b[A\n",
      " 87%|████████▋ | 356681728/407873900 [03:05<00:16, 3163232.28B/s]\u001b[A\n",
      " 88%|████████▊ | 357000192/407873900 [03:05<00:16, 3094537.66B/s]\u001b[A\n",
      " 88%|████████▊ | 357353472/407873900 [03:06<00:16, 3154522.99B/s]\u001b[A\n",
      " 88%|████████▊ | 357730304/407873900 [03:06<00:15, 3309626.13B/s]\u001b[A\n",
      " 88%|████████▊ | 358065152/407873900 [03:06<00:15, 3258913.32B/s]\u001b[A\n",
      " 88%|████████▊ | 358418432/407873900 [03:06<00:15, 3287772.01B/s]\u001b[A\n",
      " 88%|████████▊ | 358749184/407873900 [03:06<00:15, 3107139.61B/s]\u001b[A\n",
      " 88%|████████▊ | 359106560/407873900 [03:06<00:15, 3204860.32B/s]\u001b[A\n",
      " 88%|████████▊ | 359483392/407873900 [03:06<00:14, 3330534.69B/s]\u001b[A\n",
      " 88%|████████▊ | 359820288/407873900 [03:06<00:14, 3277878.16B/s]\u001b[A\n",
      " 88%|████████▊ | 360171520/407873900 [03:06<00:14, 3293268.66B/s]\u001b[A\n",
      " 88%|████████▊ | 360503296/407873900 [03:07<00:15, 3103908.30B/s]\u001b[A\n",
      " 88%|████████▊ | 360876032/407873900 [03:07<00:14, 3261650.46B/s]\u001b[A\n",
      " 89%|████████▊ | 361236480/407873900 [03:07<00:14, 3305307.65B/s]\u001b[A\n",
      " 89%|████████▊ | 361571328/407873900 [03:07<00:14, 3303380.71B/s]\u001b[A\n",
      " 89%|████████▊ | 361924608/407873900 [03:07<00:13, 3331728.44B/s]\u001b[A\n",
      " 89%|████████▉ | 362260480/407873900 [03:07<00:14, 3202991.72B/s]\u001b[A\n",
      " 89%|████████▉ | 362596352/407873900 [03:07<00:14, 3233336.19B/s]\u001b[A\n",
      " 89%|████████▉ | 362989568/407873900 [03:07<00:13, 3385354.44B/s]\u001b[A\n",
      " 89%|████████▉ | 363331584/407873900 [03:08<00:24, 1851571.82B/s]\u001b[A\n",
      " 89%|████████▉ | 364447744/407873900 [03:08<00:17, 2460577.25B/s]\u001b[A\n",
      " 89%|████████▉ | 364951552/407873900 [03:08<00:17, 2385195.37B/s]\u001b[A\n",
      " 90%|████████▉ | 365371392/407873900 [03:08<00:18, 2321477.54B/s]\u001b[A\n",
      " 90%|████████▉ | 365730816/407873900 [03:08<00:16, 2530198.31B/s]\u001b[A\n",
      " 90%|████████▉ | 366078976/407873900 [03:08<00:17, 2404518.82B/s]\u001b[A\n",
      " 90%|████████▉ | 366397440/407873900 [03:09<00:16, 2570679.64B/s]\u001b[A\n",
      " 90%|████████▉ | 366705664/407873900 [03:09<00:17, 2376000.09B/s]\u001b[A\n",
      " 90%|████████▉ | 367077376/407873900 [03:09<00:15, 2664055.93B/s]\u001b[A\n",
      " 90%|█████████ | 367382528/407873900 [03:09<00:17, 2345251.44B/s]\u001b[A\n",
      " 90%|█████████ | 367785984/407873900 [03:09<00:14, 2682098.85B/s]\u001b[A\n",
      " 90%|█████████ | 368095232/407873900 [03:09<00:15, 2523601.52B/s]\u001b[A\n",
      " 90%|█████████ | 368396288/407873900 [03:09<00:15, 2622856.35B/s]\u001b[A\n",
      " 90%|█████████ | 368710656/407873900 [03:09<00:14, 2759897.60B/s]\u001b[A\n",
      " 90%|█████████ | 369004544/407873900 [03:10<00:14, 2671416.65B/s]\u001b[A\n",
      " 91%|█████████ | 369346560/407873900 [03:10<00:13, 2838038.81B/s]\u001b[A\n",
      " 91%|█████████ | 369642496/407873900 [03:10<00:15, 2532213.02B/s]\u001b[A\n",
      " 91%|█████████ | 370067456/407873900 [03:10<00:13, 2862263.10B/s]\u001b[A\n",
      " 91%|█████████ | 370378752/407873900 [03:10<00:22, 1689534.44B/s]\u001b[A\n",
      " 91%|█████████ | 371279872/407873900 [03:10<00:16, 2230164.39B/s]\u001b[A\n",
      " 91%|█████████ | 371715072/407873900 [03:11<00:17, 2089678.55B/s]\u001b[A\n",
      " 91%|█████████ | 372074496/407873900 [03:11<00:16, 2218198.02B/s]\u001b[A\n",
      " 91%|█████████▏| 372404224/407873900 [03:11<00:16, 2206519.85B/s]\u001b[A\n",
      " 91%|█████████▏| 372700160/407873900 [03:11<00:16, 2186271.48B/s]\u001b[A\n",
      " 91%|█████████▏| 372971520/407873900 [03:11<00:15, 2191790.96B/s]\u001b[A\n",
      " 92%|█████████▏| 373227520/407873900 [03:11<00:24, 1386543.54B/s]\u001b[A\n",
      " 92%|█████████▏| 373835776/407873900 [03:12<00:18, 1798679.83B/s]\u001b[A\n",
      " 92%|█████████▏| 374153216/407873900 [03:12<00:19, 1745799.87B/s]\u001b[A\n",
      " 92%|█████████▏| 374424576/407873900 [03:12<00:19, 1723427.39B/s]\u001b[A\n",
      " 92%|█████████▏| 374665216/407873900 [03:12<00:19, 1668557.63B/s]\u001b[A\n",
      " 92%|█████████▏| 374880256/407873900 [03:12<00:19, 1669205.72B/s]\u001b[A\n",
      " 92%|█████████▏| 375080960/407873900 [03:12<00:19, 1640400.65B/s]\u001b[A\n",
      " 92%|█████████▏| 375269376/407873900 [03:12<00:19, 1657864.18B/s]\u001b[A\n",
      " 92%|█████████▏| 375451648/407873900 [03:13<00:19, 1684234.47B/s]\u001b[A\n",
      " 92%|█████████▏| 375631872/407873900 [03:13<00:19, 1681594.95B/s]\u001b[A\n",
      " 92%|█████████▏| 375809024/407873900 [03:13<00:18, 1689727.58B/s]\u001b[A\n",
      " 92%|█████████▏| 376014848/407873900 [03:13<00:18, 1694493.53B/s]\u001b[A\n",
      " 92%|█████████▏| 376211456/407873900 [03:13<00:17, 1764422.89B/s]\u001b[A\n",
      " 92%|█████████▏| 376391680/407873900 [03:13<00:17, 1755635.42B/s]\u001b[A\n",
      " 92%|█████████▏| 376569856/407873900 [03:13<00:17, 1749179.05B/s]\u001b[A\n",
      " 92%|█████████▏| 376747008/407873900 [03:13<00:17, 1746006.88B/s]\u001b[A\n",
      " 92%|█████████▏| 376965120/407873900 [03:13<00:17, 1751203.22B/s]\u001b[A\n",
      " 92%|█████████▏| 377161728/407873900 [03:14<00:17, 1804074.92B/s]\u001b[A\n",
      " 93%|█████████▎| 377344000/407873900 [03:14<00:16, 1803202.02B/s]\u001b[A\n",
      " 93%|█████████▎| 377525248/407873900 [03:14<00:17, 1770072.08B/s]\u001b[A\n",
      " 93%|█████████▎| 377718784/407873900 [03:14<00:16, 1787350.36B/s]\u001b[A\n",
      " 93%|█████████▎| 377931776/407873900 [03:14<00:16, 1848093.03B/s]\u001b[A\n",
      " 93%|█████████▎| 378118144/407873900 [03:14<00:16, 1827837.38B/s]\u001b[A\n",
      " 93%|█████████▎| 378324992/407873900 [03:14<00:16, 1823421.31B/s]\u001b[A\n",
      " 93%|█████████▎| 378521600/407873900 [03:14<00:17, 1672521.47B/s]\u001b[A\n",
      " 93%|█████████▎| 378767360/407873900 [03:14<00:15, 1841563.88B/s]\u001b[A\n",
      " 93%|█████████▎| 378963968/407873900 [03:14<00:15, 1862101.82B/s]\u001b[A\n",
      " 93%|█████████▎| 379176960/407873900 [03:15<00:14, 1923792.37B/s]\u001b[A\n",
      " 93%|█████████▎| 379373568/407873900 [03:15<00:15, 1899294.15B/s]\u001b[A\n",
      " 93%|█████████▎| 379567104/407873900 [03:15<00:15, 1772669.56B/s]\u001b[A\n",
      " 93%|█████████▎| 379774976/407873900 [03:15<00:15, 1854351.03B/s]\u001b[A\n",
      " 93%|█████████▎| 379979776/407873900 [03:15<00:14, 1886617.22B/s]\u001b[A\n",
      " 93%|█████████▎| 380192768/407873900 [03:15<00:14, 1906561.14B/s]\u001b[A\n",
      " 93%|█████████▎| 380422144/407873900 [03:15<00:13, 1967954.18B/s]\u001b[A\n",
      " 93%|█████████▎| 380620800/407873900 [03:15<00:15, 1808912.16B/s]\u001b[A\n",
      " 93%|█████████▎| 380848128/407873900 [03:15<00:14, 1891720.89B/s]\u001b[A\n",
      " 93%|█████████▎| 381041664/407873900 [03:16<00:14, 1806431.68B/s]\u001b[A\n",
      " 93%|█████████▎| 381257728/407873900 [03:16<00:14, 1875899.46B/s]\u001b[A\n",
      " 94%|█████████▎| 381460480/407873900 [03:16<00:13, 1918853.02B/s]\u001b[A\n",
      " 94%|█████████▎| 381655040/407873900 [03:16<00:13, 1885714.25B/s]\u001b[A\n",
      " 94%|█████████▎| 381863936/407873900 [03:16<00:13, 1908939.37B/s]\u001b[A\n",
      " 94%|█████████▎| 382056448/407873900 [03:16<00:14, 1825749.18B/s]\u001b[A\n",
      " 94%|█████████▎| 382273536/407873900 [03:16<00:13, 1879706.78B/s]\u001b[A\n",
      " 94%|█████████▍| 382486528/407873900 [03:16<00:13, 1947419.96B/s]\u001b[A\n",
      " 94%|█████████▍| 382683136/407873900 [03:16<00:13, 1858592.35B/s]\u001b[A\n",
      " 94%|█████████▍| 382912512/407873900 [03:17<00:13, 1907064.59B/s]\u001b[A\n",
      " 94%|█████████▍| 383105024/407873900 [03:17<00:13, 1818879.01B/s]\u001b[A\n",
      " 94%|█████████▍| 383322112/407873900 [03:17<00:13, 1873379.66B/s]\u001b[A\n",
      " 94%|█████████▍| 383551488/407873900 [03:17<00:12, 1923186.61B/s]\u001b[A\n",
      " 94%|█████████▍| 383764480/407873900 [03:17<00:12, 1942312.77B/s]\u001b[A\n",
      " 94%|█████████▍| 383977472/407873900 [03:17<00:12, 1982107.67B/s]\u001b[A\n",
      " 94%|█████████▍| 384177152/407873900 [03:17<00:12, 1863222.77B/s]\u001b[A\n",
      " 94%|█████████▍| 384387072/407873900 [03:17<00:12, 1911117.18B/s]\u001b[A\n",
      " 94%|█████████▍| 384583680/407873900 [03:17<00:13, 1712239.10B/s]\u001b[A\n",
      " 94%|█████████▍| 384829440/407873900 [03:18<00:12, 1878019.88B/s]\u001b[A\n",
      " 94%|█████████▍| 385026048/407873900 [03:18<00:12, 1903030.87B/s]\u001b[A\n",
      " 94%|█████████▍| 385222656/407873900 [03:18<00:11, 1914888.72B/s]\u001b[A\n",
      " 94%|█████████▍| 385419264/407873900 [03:18<00:11, 1896673.70B/s]\u001b[A\n",
      " 95%|█████████▍| 385612800/407873900 [03:18<00:11, 1861612.63B/s]\u001b[A\n",
      " 95%|█████████▍| 385828864/407873900 [03:18<00:11, 1866528.60B/s]\u001b[A\n",
      " 95%|█████████▍| 386041856/407873900 [03:18<00:11, 1927576.67B/s]\u001b[A\n",
      " 95%|█████████▍| 386238464/407873900 [03:18<00:11, 1891517.22B/s]\u001b[A\n",
      " 95%|█████████▍| 386428928/407873900 [03:19<00:18, 1153527.98B/s]\u001b[A\n",
      " 95%|█████████▍| 386926592/407873900 [03:19<00:14, 1486895.09B/s]\u001b[A\n",
      " 95%|█████████▍| 387170304/407873900 [03:19<00:14, 1430081.60B/s]\u001b[A\n",
      " 95%|█████████▍| 387380224/407873900 [03:19<00:14, 1407628.86B/s]\u001b[A\n",
      " 95%|█████████▌| 387567616/407873900 [03:19<00:14, 1436001.07B/s]\u001b[A\n",
      " 95%|█████████▌| 387744768/407873900 [03:19<00:14, 1381778.71B/s]\u001b[A\n",
      " 95%|█████████▌| 387909632/407873900 [03:19<00:13, 1439527.12B/s]\u001b[A\n",
      " 95%|█████████▌| 388089856/407873900 [03:20<00:13, 1497490.55B/s]\u001b[A\n",
      " 95%|█████████▌| 388253696/407873900 [03:20<00:13, 1499697.03B/s]\u001b[A\n",
      " 95%|█████████▌| 388417536/407873900 [03:20<00:12, 1534299.86B/s]\u001b[A\n",
      " 95%|█████████▌| 388578304/407873900 [03:20<00:12, 1489813.23B/s]\u001b[A\n",
      " 95%|█████████▌| 388745216/407873900 [03:20<00:12, 1528722.05B/s]\u001b[A\n",
      " 95%|█████████▌| 388925440/407873900 [03:20<00:12, 1577550.41B/s]\u001b[A\n",
      " 95%|█████████▌| 389105664/407873900 [03:20<00:11, 1607307.57B/s]\u001b[A\n",
      " 95%|█████████▌| 389269504/407873900 [03:20<00:11, 1609392.27B/s]\u001b[A\n",
      " 95%|█████████▌| 389432320/407873900 [03:20<00:11, 1549798.58B/s]\u001b[A\n",
      " 96%|█████████▌| 389621760/407873900 [03:21<00:11, 1639123.88B/s]\u001b[A\n",
      " 96%|█████████▌| 389826560/407873900 [03:21<00:10, 1667828.24B/s]\u001b[A\n",
      " 96%|█████████▌| 390006784/407873900 [03:21<00:10, 1691477.36B/s]\u001b[A\n",
      " 96%|█████████▌| 390187008/407873900 [03:21<00:10, 1694415.26B/s]\u001b[A\n",
      " 96%|█████████▌| 390358016/407873900 [03:21<00:10, 1649901.51B/s]\u001b[A\n",
      " 96%|█████████▌| 390547456/407873900 [03:21<00:10, 1679211.54B/s]\u001b[A\n",
      " 96%|█████████▌| 390744064/407873900 [03:21<00:09, 1741666.63B/s]\u001b[A\n",
      " 96%|█████████▌| 390940672/407873900 [03:21<00:09, 1781897.49B/s]\u001b[A\n",
      " 96%|█████████▌| 391120896/407873900 [03:21<00:09, 1758907.94B/s]\u001b[A\n",
      " 96%|█████████▌| 391298048/407873900 [03:21<00:09, 1703979.94B/s]\u001b[A\n",
      " 96%|█████████▌| 391497728/407873900 [03:22<00:09, 1744439.68B/s]\u001b[A\n",
      " 96%|█████████▌| 391710720/407873900 [03:22<00:08, 1805736.37B/s]\u001b[A\n",
      " 96%|█████████▌| 391907328/407873900 [03:22<00:08, 1831325.61B/s]\u001b[A\n",
      " 96%|█████████▌| 392091648/407873900 [03:22<00:08, 1812544.11B/s]\u001b[A\n",
      " 96%|█████████▌| 392273920/407873900 [03:22<00:08, 1747711.36B/s]\u001b[A\n",
      " 96%|█████████▌| 392480768/407873900 [03:22<00:08, 1796974.82B/s]\u001b[A\n",
      " 96%|█████████▋| 392693760/407873900 [03:22<00:08, 1848073.90B/s]\u001b[A\n",
      " 96%|█████████▋| 392906752/407873900 [03:22<00:08, 1824645.78B/s]\u001b[A\n",
      " 96%|█████████▋| 393136128/407873900 [03:22<00:07, 1939312.27B/s]\u001b[A\n",
      " 96%|█████████▋| 393332736/407873900 [03:23<00:07, 1850368.99B/s]\u001b[A\n",
      " 96%|█████████▋| 393521152/407873900 [03:23<00:07, 1840081.71B/s]\u001b[A\n",
      " 97%|█████████▋| 393709568/407873900 [03:23<00:07, 1816590.83B/s]\u001b[A\n",
      " 97%|█████████▋| 393906176/407873900 [03:23<00:07, 1843290.60B/s]\u001b[A\n",
      " 97%|█████████▋| 394135552/407873900 [03:23<00:07, 1944139.96B/s]\u001b[A\n",
      " 97%|█████████▋| 394333184/407873900 [03:23<00:07, 1866306.81B/s]\u001b[A\n",
      " 97%|█████████▋| 394522624/407873900 [03:23<00:07, 1853461.92B/s]\u001b[A\n",
      " 97%|█████████▋| 394725376/407873900 [03:23<00:07, 1843422.74B/s]\u001b[A\n",
      " 97%|█████████▋| 394921984/407873900 [03:23<00:07, 1849067.89B/s]\u001b[A\n",
      " 97%|█████████▋| 395151360/407873900 [03:24<00:06, 1959732.11B/s]\u001b[A\n",
      " 97%|█████████▋| 395350016/407873900 [03:24<00:06, 1884370.39B/s]\u001b[A\n",
      " 97%|█████████▋| 395541504/407873900 [03:24<00:06, 1887578.07B/s]\u001b[A\n",
      " 97%|█████████▋| 395732992/407873900 [03:24<00:06, 1895586.42B/s]\u001b[A\n",
      " 97%|█████████▋| 395937792/407873900 [03:24<00:06, 1830901.82B/s]\u001b[A\n",
      " 97%|█████████▋| 396183552/407873900 [03:24<00:05, 1969783.83B/s]\u001b[A\n",
      " 97%|█████████▋| 396385280/407873900 [03:24<00:06, 1797286.85B/s]\u001b[A\n",
      " 97%|█████████▋| 396571648/407873900 [03:25<00:09, 1145048.40B/s]\u001b[A\n",
      " 97%|█████████▋| 397108224/407873900 [03:25<00:07, 1498705.38B/s]\u001b[A\n",
      " 97%|█████████▋| 397373440/407873900 [03:25<00:07, 1409719.46B/s]\u001b[A\n",
      " 97%|█████████▋| 397596672/407873900 [03:25<00:07, 1411919.52B/s]\u001b[A\n",
      " 98%|█████████▊| 397795328/407873900 [03:25<00:06, 1461082.61B/s]\u001b[A\n",
      " 98%|█████████▊| 397982720/407873900 [03:25<00:07, 1411912.07B/s]\u001b[A\n",
      " 98%|█████████▊| 398152704/407873900 [03:25<00:06, 1417152.99B/s]\u001b[A\n",
      " 98%|█████████▊| 398314496/407873900 [03:26<00:06, 1374009.53B/s]\u001b[A\n",
      " 98%|█████████▊| 398510080/407873900 [03:26<00:06, 1502414.36B/s]\u001b[A\n",
      " 98%|█████████▊| 398690304/407873900 [03:26<00:05, 1576090.09B/s]\u001b[A\n",
      " 98%|█████████▊| 398859264/407873900 [03:26<00:05, 1541229.98B/s]\u001b[A\n",
      " 98%|█████████▊| 399050752/407873900 [03:26<00:05, 1583835.93B/s]\u001b[A\n",
      " 98%|█████████▊| 399215616/407873900 [03:26<00:05, 1507401.14B/s]\u001b[A\n",
      " 98%|█████████▊| 399411200/407873900 [03:26<00:05, 1555727.82B/s]\u001b[A\n",
      " 98%|█████████▊| 399591424/407873900 [03:26<00:05, 1588019.96B/s]\u001b[A\n",
      " 98%|█████████▊| 399755264/407873900 [03:26<00:05, 1594696.17B/s]\u001b[A\n",
      " 98%|█████████▊| 399984640/407873900 [03:26<00:04, 1733439.93B/s]\u001b[A\n",
      " 98%|█████████▊| 400163840/407873900 [03:27<00:04, 1623933.20B/s]\u001b[A\n",
      " 98%|█████████▊| 400331776/407873900 [03:27<00:04, 1619506.52B/s]\u001b[A\n",
      " 98%|█████████▊| 400508928/407873900 [03:27<00:04, 1570205.06B/s]\u001b[A\n",
      " 98%|█████████▊| 400738304/407873900 [03:27<00:04, 1722001.08B/s]\u001b[A\n",
      " 98%|█████████▊| 400951296/407873900 [03:27<00:03, 1746695.95B/s]\u001b[A\n",
      " 98%|█████████▊| 401131520/407873900 [03:27<00:03, 1738124.47B/s]\u001b[A\n",
      " 98%|█████████▊| 401344512/407873900 [03:27<00:03, 1815945.77B/s]\u001b[A\n",
      " 98%|█████████▊| 401529856/407873900 [03:27<00:03, 1676447.94B/s]\u001b[A\n",
      " 98%|█████████▊| 401737728/407873900 [03:28<00:03, 1746959.69B/s]\u001b[A\n",
      " 99%|█████████▊| 401934336/407873900 [03:28<00:03, 1707065.80B/s]\u001b[A\n",
      " 99%|█████████▊| 402130944/407873900 [03:28<00:03, 1771736.64B/s]\u001b[A\n",
      " 99%|█████████▊| 402376704/407873900 [03:28<00:02, 1923472.69B/s]\u001b[A\n",
      " 99%|█████████▊| 402575360/407873900 [03:28<00:02, 1797255.50B/s]\u001b[A\n",
      " 99%|█████████▊| 402760704/407873900 [03:31<00:27, 189265.24B/s] \u001b[A\n",
      " 99%|█████████▉| 402892800/407873900 [03:31<00:21, 232477.12B/s]\u001b[A\n",
      " 99%|█████████▉| 403048448/407873900 [03:32<00:17, 280813.38B/s]\u001b[A\n",
      " 99%|█████████▉| 403140608/407873900 [03:32<00:15, 303815.46B/s]\u001b[A\n",
      " 99%|█████████▉| 403671040/407873900 [03:32<00:10, 407492.55B/s]\u001b[A\n",
      " 99%|█████████▉| 403916800/407873900 [03:32<00:08, 488367.87B/s]\u001b[A\n",
      " 99%|█████████▉| 404162560/407873900 [03:33<00:06, 570916.41B/s]\u001b[A\n",
      " 99%|█████████▉| 404424704/407873900 [03:33<00:05, 651196.72B/s]\u001b[A\n",
      " 99%|█████████▉| 404686848/407873900 [03:33<00:04, 728126.91B/s]\u001b[A\n",
      " 99%|█████████▉| 404965376/407873900 [03:33<00:03, 798039.80B/s]\u001b[A\n",
      " 99%|█████████▉| 405243904/407873900 [03:34<00:03, 847712.27B/s]\u001b[A\n",
      " 99%|█████████▉| 405522432/407873900 [03:34<00:02, 912388.28B/s]\u001b[A\n",
      " 99%|█████████▉| 405817344/407873900 [03:34<00:02, 944565.38B/s]\u001b[A\n",
      "100%|█████████▉| 406095872/407873900 [03:34<00:01, 989838.59B/s]\u001b[A\n",
      "100%|█████████▉| 406390784/407873900 [03:35<00:01, 1006448.39B/s]\u001b[A\n",
      "100%|█████████▉| 406669312/407873900 [03:35<00:01, 1157969.05B/s]\u001b[A\n",
      "100%|█████████▉| 406797312/407873900 [03:35<00:01, 1068488.62B/s]\u001b[A\n",
      "100%|█████████▉| 406914048/407873900 [03:35<00:01, 733862.51B/s] \u001b[A\n",
      "100%|█████████▉| 407111680/407873900 [03:36<00:00, 790925.09B/s]\u001b[A\n",
      "100%|█████████▉| 407308288/407873900 [03:36<00:00, 864111.37B/s]\u001b[A\n",
      "100%|█████████▉| 407408640/407873900 [03:36<00:00, 814794.83B/s]\u001b[A\n",
      "100%|█████████▉| 407537664/407873900 [03:36<00:00, 737802.60B/s]\u001b[A\n",
      "100%|█████████▉| 407767040/407873900 [03:36<00:00, 840994.01B/s]\u001b[A\n",
      "100%|██████████| 407873900/407873900 [03:36<00:00, 1880808.10B/s]\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[ATrain loss: 0.0942140182803485\n",
      "Epoch:   0%|          | 0/4 [01:10<?, ?it/s]Validation loss: 0.06860410242720887\n",
      "Validation Accuracy: 0.979375285779607\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Found input variables without list of list.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-934434e5a337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mpred_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtags_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mvalid_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtags_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_ii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml_ii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1-Score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/envs/zalo_chanhoa/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, average, suffix, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[1;32m    363\u001b[0m                                                      \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                                                      \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                                                      suffix=suffix)\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/zalo_chanhoa/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, suffix)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mscheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/zalo_chanhoa/lib/python3.6/site-packages/seqeval/metrics/v1.py\u001b[0m in \u001b[0;36m_precision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, scheme, suffix, extract_tp_actual_correct)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'average has to be one of {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mpred_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/zalo_chanhoa/lib/python3.6/site-packages/seqeval/metrics/v1.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mis_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found input variables without list of list.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen_true\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Found input variables without list of list."
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.04753524796278388\n",
      "Validation Accuracy: 0.9848725422953818\n",
      "Validation F1-Score: 0.5591739475774424\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get keywords from sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordextract(sentence):\n",
    "    text = sentence\n",
    "    tkns = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tkns)\n",
    "    segments_ids = [0] * len(tkns)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "    segments_tensors = torch.tensor([segments_ids]).to(device)\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    logit = model(tokens_tensor, token_type_ids=None,\n",
    "                                  attention_mask=segments_tensors)\n",
    "    logit = logit.detach().cpu().numpy()\n",
    "    prediction.extend([list(p) for p in np.argmax(logit, axis=2)])\n",
    "    for k, j in enumerate(prediction[0]):\n",
    "        if j==1 or j==0:\n",
    "            print(tokenizer.convert_ids_to_tokens(tokens_tensor[0].to('cpu').numpy())[k], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The solution is based upon an abstract representation of the mobile object system.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobile 0\n"
     ]
    }
   ],
   "source": [
    "keywordextract(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}